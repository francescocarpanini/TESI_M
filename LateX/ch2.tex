\NoBgThispage
\chapter{MapAlign: Training and Model Approaches}

This chapter provides a detailed and comprehensive analysis of the developed architecture throughout its various phases, focusing particularly on the two approaches that reached the most promising results.

\section{First Approach}

The initial approach involved designing a model to perform the task described in previous chapters without directly using the images. Instead of using raw images, the model directly worked with features extracted from them, focusing on the pre-existing obstacle reconstructions, by resulting in a significantly lighter input tensor, as all the data from the images was missing.

The \textit{BEVPreprocessor} prepares the input data with a simple structure consisting of a single convolutional layer with just $0.0013$ million parameters, keeping it efficient. 
The \textit{Backbone}, based on the BiSeNetV1 architecture \cite{DBLP:journals/corr/abs-1808-00897}, is the most complex component, with $12.63$ million parameters. The backbone uses two paths: 
\begin{enumerate}
    \item Spatial Path: Designed with a small stride to preserve spatial details, this path captures high-resolution features through several convolutional layers, emphasizing fine-grained spatial information.
    \item Context Path: This path quickly down-samples the input to expand the receptive field, helping the model capture broader context. It utilizes Attention Refinement Modules (ARMs) and additional convolutional layers. ARMs allow the model to focus on important regions within the image, significantly contributing to the model's parameter count.
\end{enumerate}
A Feature Fusion Module is used to efficiently combine the features from the Spatial and Context Paths.

The \textit{Decoder} then up-samples these features to restore them to the original input resolution, with a parameter count of $0.0924$ million. The role of the decoder is to convert high-level, low-resolution features back into the detailed, high-resolution format needed for accurate output. This component ensures that the model’s outputs match the input image's scale, making the model suitable for precise localization tasks.

The head, called \textit{PoseSegHead}, generates the segmentation output, while a fully connected (Fc) layer finalizes the pose estimation. This design enables efficient and accurate pose analysis. The model’s total parameter count is optimized to balance processing efficiency and accuracy.

Figure \ref{fig:enter-label12} illustrates the complete model structure.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{LateX//figs/modello_POSENET.pdf}
    \caption{Structure of the PoseNet Model INSERIRE SCHEMA}
    \label{fig:enter-label12}
\end{figure}

The primary goal was to accurately predict the four values that allow the alignmente between the car pose and the HD map: translations along the \( x, y, z \) axes and the heading angle. Alongside model design, appropriate loss functions were also defined. As described in Chapter 1, the three loss functions used were L1, L1-smooth, and MSE.
It is advantageous to employ a single loss function that accounts for all parameters collectively, as this ensures the network learns to optimize the relationships and trade-offs between them in a holistic manner. However, incorporating individual loss components for each parameter can further enhance the model's performance by encouraging it to focus on specific prediction tasks. This approach can improve interpretability, as it allows the network to account for parameter-specific nuances, and promote generalization by preventing the dominance of any single parameter in the optimization process. For this reason two additional specialized losses for the translation and heading angle were added. 
The heading angle was especially critical and challenging to learn accurately, so this structure was consistent across different model versions.

The initial iterations used the following high-level parameters:
\begin{itemize}
    \item Data augmentation: Included body rotations between $-30^\circ$ and $30^\circ$, and horizontal and vertical shifts up to $10\%$.
    \item Optimizer: Training used the SGD optimizer with a base learning rate of \textit{0.1} and a batch size of \textit{64}.
    \item Learning Rate Scheduler: The scheduler was WarmupPolyLR \cite{kalra2024warmuplearningrateunderlying} with a warm-up period of $5000$ iterations, capped at a maximum of $100000$ iterations.
    \item Automatic Mixed Precision (AMP) was enabled to improve training speed and resource use.
\end{itemize}

The learning rate demonstrated the following behavior during training:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{LateX//figs/learning_rate_progression.png}
    \caption{Learning Rate Progression During Training}
    \label{fig:learning-rate-progression}
\end{figure}

Checkpoints were saved every $100$ iterations, and evaluations were performed every $5000$ iterations using the \textit{PoseEvaluator}. The evaluator calculated the L1 loss on a validation set to objectively assess the model’s performance. The loss was computed as the absolute difference between the ground truth and predicted values for position and heading, as shown below:
\begin{align}
    \text{eval}_x &= |x_{\text{gt}} - x_{\text{pred}}|, \\
    \text{eval}_y &= |y_{\text{gt}} - y_{\text{pred}}|, \\
    \text{eval}_z &= |z_{\text{gt}} - z_{\text{pred}}|, \\
    \text{eval}_{\theta} &= |\theta_{\text{gt}} - \theta_{\text{pred}}|.
\end{align}

Training was accelerated with CUDA, using CuDNN benchmarking for improved speed. Initial training sessions utilized only a portion of the dataset to refine the model’s structure and functionality before scaling to the full dataset.

\subsubsection*{MSE}
The first training session of the architecture described used MSE as the loss function, along with the parameters described above. This approach resulted in a steadily decreasing loss, as shown in the figure below:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{LateX//figs/loss_total_mse_progression_comparison.png}
    \caption{Total Loss Progression Using MSE as the Loss Function}
    \label{fig:mse-loss-progression}
\end{figure}

Focusing on specific losses for the pose and heading angle, the following behavior was observed:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{LateX//figs/mse_pose_heading_loss_comparison.png}
    \caption{Pose and Heading Loss Comparison During Training}
    \label{fig:pose-heading-loss}
\end{figure}

The model's behavior during evaluation aligned with expectations, and the training evaluations yielded the following results:
\begin{table}[H]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{3.25cm} >{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{2.25cm}}
        \toprule
        \textbf{Iteration} & \textbf{$\Delta$ Pose} & \textbf{$\Delta \theta$} & \textbf{$\Delta x + \Delta y$} & \textbf{$\Delta h$} \\
        & \text{[m]} & \text{[rad] $\rightarrow$ [deg]} & \text{[m]} & \text{[m]} \\
        \midrule
        \num{20000} & 2.02 & 0.05 $\rightarrow$ 2.86 & 1.94 & 0.08 \\
        \num{40000} & 1.43 & 0.22 $\rightarrow$ 12.6 & 1.37 & 0.06 \\
        \num{60000} & 1.42 & 0.04 $\rightarrow$ 2.29 & 1.39 & 0.02 \\
        \num{80000} & 1.99 & 0.02 $\rightarrow$ 1.15 & 1.99 & 0.00 \\
        \num{100000} & 1.54 & 0.02 $\rightarrow$ 1.15 & 1.54 & 0.00 \\
        \bottomrule
    \end{tabular}
    \caption{Variations in pose, angle, xy, and height across different iterations.}
    \label{tab:pose_variations}
\end{table}

Despite the loss functions decreasing and reaching very low values, indicating potential generalization and effective learning, the evaluation process reveals that the results are still not optimal or acceptable. The positional error exceeds one meter, which is insufficient for HD map alignment applications, as discussed in Chapter 1. This could be acceptable if iterative optimization is still included in the desired pipeline of the dataset generator. This approach could simply eliminate the part where the sequences had to be manually aligned. However, it was not sufficient, so other attempts were made and will be further demonstrated. 

\subsection*{L1 Loss}
In this other example, the total loss was supplemented by two specific auxiliary losses using the L1 loss, defined similarly to the previous set-up. The results with the L1 loss are consistent with previous observations. 
The figure below illustrates the behavior of the neural network. Vertically, the first three images represent the inputs: obstacles extracted directly on the car, associated HD maps, and the fusion of the two. The fourth image represents the target, which is the perfect alignment that the network aims to achieve. The last image represents the prediction by the network. As can be seen, this time, the prediction is post-processed to perform a roto-translation of the car and its world reconstruction.

As the results demonstrate, although the measurements were taken at two distinct points during training, one at the beginning and one at the end, they are not entirely accurate. Nevertheless, a noticeable improvement in the heading angle can be observed. 
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{LateX//figs/IMMAGINI_L1_rete.pdf}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

Since the specific loss graphs did not reveal significant differences, only the results table is provided for this experiment.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{LateX//figs/tabella_l1.png}
    \caption{Evaluation Results with L1 Loss-function}
    \label{fig:l1-loss-results}
\end{figure}
\begin{table}[H]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{3.25cm} >{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{2.25cm}}
        \toprule
        \textbf{Iteration} & \textbf{$\Delta$ Pose} & \textbf{$\Delta \theta$} & \textbf{$\Delta x + \Delta y$} & \textbf{$\Delta h$} \\
        & \text{[m]} & \text{[rad] $\rightarrow$ [deg]} & \text{[m]} & \text{[m]} \\
        \midrule
        \num{20000} & 2.00 & 0.11 $\rightarrow$ 6.31 & 1.98 & 0.02 \\
        \num{40000} & 1.89 & 0.13 $\rightarrow$ 7.45 & 1.84 & 0.05 \\
        \num{60000} & 1.45 & 0.04 $\rightarrow$ 2.29 & 1.43 & 0.02 \\
        \num{80000} & 1.42 & 0.04 $\rightarrow$ 2.29 & 1.40 & 0.02 \\
        \num{100000} & 1.40 & 0.03 $\rightarrow$ 1.72 & 1.40 & 0.00 \\
        \bottomrule
    \end{tabular}
    \caption{Variations in pose, angle, xy, and height across different iterations.}
    \label{tab:pose_variations}
\end{table}

\subsection*{Smooth L1 Loss}
In this experiment, the Smooth L1 loss, similar to L1 but less sensitive to outliers, was used. The training results remained consistent with expectations, but this time some interesting differences emerged, as the results showed slight improvements over previous attempts. Both loss graphs and the evaluation table are provided below to illustrate this behavior.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{LateX//figs/l1s_pose_heading_loss_comparison.png}
    \caption{Pose and Heading Loss Comparison Using Smooth L1 Loss}
    \label{fig:smooth-l1-pose-heading-loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{LateX//figs/loss_total_l1s_progression_comparison.png}
    \caption{Total Loss Progression Using Smooth L1 Loss}
    \label{fig:smooth-l1-total-loss}
\end{figure}



A summary of the evaluation results during training is shown below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Screenshot 2024-11-14 at 11.50.25.png}
    \caption{Summary of Evaluation Results for Smooth L1 Loss (Table to be updated)}
    \label{fig:smooth-l1-evaluation}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{3.25cm} >{\centering\arraybackslash}p{2.25cm} >{\centering\arraybackslash}p{2.25cm}}
        \toprule
        \textbf{Iteration} & \textbf{$\Delta$ Pose} & \textbf{$\Delta \theta$} & \textbf{$\Delta x + \Delta y$} & \textbf{$\Delta h$} \\
        & \text{[m]} & \text{[rad] $\rightarrow$ [deg]} & \text{[m]} & \text{[m]} \\
        \midrule
        \num{20000} & 1.75 & 0.14 $\rightarrow$ 8.02 & 1.72 & 0.02 \\
        \num{40000} & 1.81 & 0.14 $\rightarrow$ 8.02 & 1.72 & 0.02 \\
        \num{60000} & 1.35 & 0.02 $\rightarrow$ 1.15 & 1.36 & 0.01 \\
        \num{80000} & 1.46 & 0.02 $\rightarrow$ 1.15 & 1.46 & 0.00 \\
        \num{100000} & 1.38 & 0.02 $\rightarrow$ 1.15 & 1.28 & 0.00 \\
        \bottomrule
    \end{tabular}
    \caption{Variations in pose, angle, xy, and height across different iterations.}
    \label{tab:pose_variations}
\end{table}

As indicated in the table, the Smooth L1 loss yielded the best results. This outcome aligns with the characteristics of Smooth L1 loss, which merges the benefits of L1 and L2 losses. It reduces sensitivity to outliers more effectively than MSE loss, while balancing convergence speed and stability. This makes it particularly suitable for regression tasks involving spatial transformations.

To ensure the network’s predictions are on a similar scale, facilitating learning, the heading angle (initially in radians) was converted to degrees. This adjustment corrected a scale imbalance, as angle values in radians were significantly smaller than the translation values. The same experiments were repeated after this adjustment, using the same loss functions, with results shown below.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Screenshot 2024-11-14 at 11.52.27.png}
    \caption{Evaluation Results with Heading Angle in Degrees (Table to be updated)}
    \label{fig:degree-evaluation-results}
\end{figure}

As expected, scaling the heading angle to degrees improved the results, reducing errors and bringing predicted values closer to desired outcomes. Once again, the Smooth L1 loss provided the best generalization performance.

These findings highlight the importance of consistent value scales across predicted parameters and the effectiveness of Smooth L1 loss in training models for spatial transformation estimation.

Given these findings, when the full dataset was utilized by merging all sequences from the three streets, only the best-performing model from the preliminary analysis (using Smooth L1 loss) was tested. The results are shown and discussed below.

\textbf{Full Dataset Results:} The graphs below show improvements in angle prediction, aided by the increased data volume, Smooth L1 loss, and the angle conversion to degrees. However, the main issue remains with translation error. Despite improvements, further refinement or the use of a more precise optimization method is needed to achieve accurate HD map alignment. This model version could potentially assist in challenging scenarios but does not fully solve the alignment problem.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{LateX//figs/l1sDEG_pose_heading_loss_comparison.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{loss_total_l1sDEG_progression_comparison.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

\subsection{Inference}
Inference was also tested on an unseen sequence that was not part of the training dataset. This sequence contained long stretches of straight road, with few distinctive textures or reference points. During inference, it appeared that the network attempted to minimize the distance but from the wrong boundary, creating good alignment for the angle but misaligning the translation by an entire lane width. An example of this behavior is shown below:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Untitled 2.pdf}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}